{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:**\n",
    "\n",
    "This is the Jupyter notebook is used to test a few classier: Logistic regression, KNN and Naive Baise (GaussianNB).\n",
    "\n",
    "**Project Name:** Lending Club\n",
    "\n",
    "**Team:** Silas Mederer, Jonas Bechthold\n",
    "\n",
    "**Date:** 2020-10-02 to 2020-10-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up & load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 100           # Ensures modeling results can be replicated\n",
    "pd.set_option('display.max_columns', 50) # Sets maximum columns displayed in tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"data/df_clean.csv\")\n",
    "df.drop(\"Unnamed: 0\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(filename=str):\n",
    "\n",
    "    print(\"Confusion matrix train\")\n",
    "    print(confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "    print(\"Classification report train\")\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "    # Figure test\n",
    "    conf_mat = pd.crosstab(np.ravel(y_test), np.ravel(y_pred),\n",
    "                           colnames=[\"Predicted\"], rownames=[\"Actual\"])\n",
    "    sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt=\"g\")\n",
    "\n",
    "    plt.title('Confusion Matrix: Test Data')\n",
    "    plt.savefig(\n",
    "        f\"plots/conf-matrix-train-{filename}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Confusion matrix test\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(\"Classification report test\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Figure train\n",
    "    conf_mat = pd.crosstab(np.ravel(y_train), np.ravel(y_train_pred),\n",
    "                           colnames=[\"Predicted\"], rownames=[\"Actual\"])\n",
    "    sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt=\"g\")\n",
    "\n",
    "    plt.title('Confusion Matrix: Training Data')\n",
    "    plt.savefig(\n",
    "        f\"plots/conf-matrix-test-{filename}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def optimization_plots(model):\n",
    "    print(plot_grid_search(model))\n",
    "    print(table_grid_search(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorial and continuos variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_var = list(df.select_dtypes('object').columns)\n",
    "print(\"categorial var\")\n",
    "print(categorical_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_var = list(df.select_dtypes('int').columns) + list(df.select_dtypes('float').columns)\n",
    "print(\"continues var\")\n",
    "print(continuous_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "mask = np.triu(df.corr())\n",
    "ax = sns.heatmap(round(df.corr()*100,0)\n",
    "                 ,annot=True\n",
    "                 ,mask=mask\n",
    "                 ,cmap=\"coolwarm\")\n",
    "plt.savefig('plots/correlogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.default==0]\n",
    "df_minority = df[df.default==1]\n",
    "\n",
    "target_sample_size = 20000\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,                    # sample with replacement\n",
    "                                 n_samples=target_sample_size,    # to match majority class\n",
    "                                 random_state=random_state)                 # reproducible results\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=True,                    # sample with replacement\n",
    "                                 n_samples=target_sample_size,    # to match majority class\n",
    "                                 random_state=random_state)                 # reproducible results\n",
    "\n",
    "print(f\"Size of minority df: {df_minority_upsampled.default.value_counts()}\")\n",
    "print(f\"Size of majority df: {df_majority_downsampled.default.value_counts()}\")\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority_downsampled, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.default.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_upsampled\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df.copy(), drop_first=True)\n",
    "X.drop(\"default\", axis=1, inplace=True)\n",
    "y = df[\"default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "# plot\n",
    "y.plot.hist()\n",
    "y_train.plot.hist()\n",
    "y_test.plot.hist()\n",
    "plt.xlabel([\"default=False\", \"default=True\"])\n",
    "plt.xticks([0, 1])\n",
    "\n",
    "\n",
    "# dataframe with relative and absolut values\n",
    "plt.legend(['all','train','test'])\n",
    "tts_df = pd.DataFrame()\n",
    "tts_df['train abs'] = round(y_train.value_counts(),2)\n",
    "tts_df['train %'] = round((y_train.value_counts()/y_train.shape[0]),2)\n",
    "tts_df['test abs'] = round(y_test.value_counts(),2)\n",
    "tts_df['test %'] = round((y_test.value_counts()/y_test.shape[0]),2)\n",
    "tts_df['all abs'] = round(y.value_counts(),2)\n",
    "tts_df['all %'] = round((y.value_counts()/y.shape[0]),2)\n",
    "tts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaling_list = [\"loan_amnt\",\"annual_inc\"]\n",
    "scaling_min_max = [\"annual_inc\"]\n",
    "\n",
    "def logarithmize(value):\n",
    "    #print(value)\n",
    "    logvalue = np.log(value)\n",
    "    return logvalue\n",
    "\n",
    "# scaling train with fit transform\n",
    "X_train[scaling_min_max] = X_train[scaling_min_max].apply(lambda x: logarithmize(x))\n",
    "X_train[scaling_list] = scaler.fit_transform(X_train[scaling_list])\n",
    "\n",
    "# scaling test with same scaling parameters as the training set (just transform)\n",
    "X_test[scaling_min_max] = X_test[scaling_min_max].apply(lambda x: logarithmize(x))\n",
    "X_test[scaling_list] = scaler.transform(X_test[scaling_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "y_train_pred = lr.predict(X_train)\n",
    "\n",
    "scores(\"log_reg_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Params came from this source: https://medium.com/@jackstalfort/hyperparameter-tuning-using-grid-search-and-random-search-f8750a464b35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Hyperparameter grid\n",
    "C = np.logspace(0, 4, num=10)\n",
    "penalty = ['l1', 'l2']\n",
    "solver = ['liblinear', 'saga']\n",
    "hyperparameters = dict(C=C, penalty=penalty, solver=solver)\n",
    "\n",
    "# Estimator for use in random search\n",
    "estimator = LogisticRegression()\n",
    "\n",
    "# Create the random search model\n",
    "logreg = RandomizedSearchCV(estimator, hyperparameters, \n",
    "                            n_jobs=-1,\n",
    "                            scoring='recall', \n",
    "                            cv=5,\n",
    "                            n_iter=25, \n",
    "                            verbose=1)\n",
    "\n",
    "# Fit\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print(logreg.best_params_)\n",
    "print(round(logreg.best_score_,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Best params on 07.October were: {'solver': 'liblinear', 'penalty': 'l1', 'C': 10000.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method down here was created by Juanma Hernández and can be found here Source: https://www.kaggle.com/juanmah/grid-search-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "\"\"\"Utility script with functions to be used with the results of GridSearchCV.\n",
    "\n",
    "**plot_grid_search** plots as many graphs as parameters are in the grid search results.\n",
    "\n",
    "**table_grid_search** shows tables with the grid search results.\n",
    "\n",
    "Inspired in [Displaying the results of a Grid Search](https://www.kaggle.com/grfiv4/displaying-the-results-of-a-grid-search) notebook,\n",
    "of [George Fisher](https://www.kaggle.com/grfiv4)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import pprint\n",
    "from scipy import stats\n",
    "from IPython.display import display\n",
    "\n",
    "__author__ = \"Juanma Hernández\"\n",
    "__copyright__ = \"Copyright 2019\"\n",
    "__credits__ = [\"Juanma Hernández\", \"George Fisher\"]\n",
    "__license__ = \"GPL\"\n",
    "__maintainer__ = \"Juanma Hernández\"\n",
    "__email__ = \"https://twitter.com/juanmah\"\n",
    "__status__ = \"Utility script\"\n",
    "\n",
    "\n",
    "def plot_grid_search(clf):\n",
    "    \"\"\"Plot as many graphs as parameters are in the grid search results.\n",
    "\n",
    "    Each graph has the values of each parameter in the X axis and the Score in the Y axis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf: estimator object result of a GridSearchCV\n",
    "        This object contains all the information of the cross validated results for all the parameters combinations.\n",
    "    \"\"\"\n",
    "    # Convert the cross validated results in a DataFrame ordered by `rank_test_score` and `mean_fit_time`.\n",
    "    # As it is frequent to have more than one combination with the same max score,\n",
    "    # the one with the least mean fit time SHALL appear first.\n",
    "    cv_results = pd.DataFrame(clf.cv_results_).sort_values(by=['rank_test_score', 'mean_fit_time'])\n",
    "\n",
    "    # Get parameters\n",
    "    parameters=cv_results['params'][0].keys()\n",
    "\n",
    "    # Calculate the number of rows and columns necessary\n",
    "    rows = -(-len(parameters) // 2)\n",
    "    columns = min(len(parameters), 2)\n",
    "    # Create the subplot\n",
    "    fig = make_subplots(rows=rows, cols=columns)\n",
    "    # Initialize row and column indexes\n",
    "    row = 1\n",
    "    column = 1\n",
    "\n",
    "    # For each of the parameters\n",
    "    for parameter in parameters:\n",
    "\n",
    "        # As all the graphs have the same traces, and by default all traces are shown in the legend,\n",
    "        # the description appears multiple times. Then, only show legend of the first graph.\n",
    "        if row == 1 and column == 1:\n",
    "            show_legend = True\n",
    "        else:\n",
    "            show_legend = False\n",
    "\n",
    "        # Mean test score\n",
    "        mean_test_score = cv_results[cv_results['rank_test_score'] != 1]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            name='Mean test score',\n",
    "            x=mean_test_score['param_' + parameter],\n",
    "            y=mean_test_score['mean_test_score'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=mean_test_score['mean_fit_time'],\n",
    "                        color='SteelBlue',\n",
    "                        sizeref=2. * cv_results['mean_fit_time'].max() / (40. ** 2),\n",
    "                        sizemin=4,\n",
    "                        sizemode='area'),\n",
    "            text=mean_test_score['params'].apply(\n",
    "                lambda x: pprint.pformat(x, width=-1).replace('{', '').replace('}', '').replace('\\n', '<br />')),\n",
    "            showlegend=show_legend),\n",
    "            row=row,\n",
    "            col=column)\n",
    "\n",
    "        # Best estimators\n",
    "        rank_1 = cv_results[cv_results['rank_test_score'] == 1]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            name='Best estimators',\n",
    "            x=rank_1['param_' + parameter],\n",
    "            y=rank_1['mean_test_score'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=rank_1['mean_fit_time'],\n",
    "                        color='Crimson',\n",
    "                        sizeref=2. * cv_results['mean_fit_time'].max() / (40. ** 2),\n",
    "                        sizemin=4,\n",
    "                        sizemode='area'),\n",
    "            text=rank_1['params'].apply(str),\n",
    "            showlegend=show_legend),\n",
    "            row=row,\n",
    "            col=column)\n",
    "\n",
    "        fig.update_xaxes(title_text=parameter, row=row, col=column)\n",
    "        fig.update_yaxes(title_text='Score', row=row, col=column)\n",
    "\n",
    "        # Check the linearity of the series\n",
    "        # Only for numeric series\n",
    "        if pd.to_numeric(cv_results['param_' + parameter], errors='coerce').notnull().all():\n",
    "            x_values = cv_results['param_' + parameter].sort_values().unique().tolist()\n",
    "            r = stats.linregress(x_values, range(0, len(x_values))).rvalue\n",
    "            # If not so linear, then represent the data as logarithmic\n",
    "            if r < 0.86:\n",
    "                fig.update_xaxes(type='log', row=row, col=column)\n",
    "\n",
    "        # Increment the row and column indexes\n",
    "        column += 1\n",
    "        if column > columns:\n",
    "            column = 1\n",
    "            row += 1\n",
    "\n",
    "            # Show first the best estimators\n",
    "    fig.update_layout(legend=dict(traceorder='reversed'),\n",
    "                      width=columns * 360 + 100,\n",
    "                      height=rows * 360,\n",
    "                      title='Best score: {:.6f} with {}'.format(cv_results['mean_test_score'].iloc[0],\n",
    "                                                                str(cv_results['params'].iloc[0]).replace('{',\n",
    "                                                                                                          '').replace(\n",
    "                                                                    '}', '')),\n",
    "                      hovermode='closest',\n",
    "                      template='none')\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def table_grid_search(clf, all_columns=False, all_ranks=False, save=True):\n",
    "    \"\"\"Show tables with the grid search results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf: estimator object result of a GridSearchCV\n",
    "        This object contains all the information of the cross validated results for all the parameters combinations.\n",
    "\n",
    "    all_columns: boolean, default: False\n",
    "        If true all columns are returned. If false, the following columns are dropped:\n",
    "\n",
    "        - params. As each parameter has a column with the value.\n",
    "        - std_*. Standard deviations.\n",
    "        - split*. Split scores.\n",
    "\n",
    "    all_ranks: boolean, default: False\n",
    "        If true all ranks are returned. If false, only the rows with rank equal to 1 are returned.\n",
    "\n",
    "    save: boolean, default: True\n",
    "        If true, results are saved to a CSV file.\n",
    "    \"\"\"\n",
    "    # Convert the cross validated results in a DataFrame ordered by `rank_test_score` and `mean_fit_time`.\n",
    "    # As it is frequent to have more than one combination with the same max score,\n",
    "    # the one with the least mean fit time SHALL appear first.\n",
    "    cv_results = pd.DataFrame(clf.cv_results_).sort_values(by=['rank_test_score', 'mean_fit_time'])\n",
    "\n",
    "    # Reorder\n",
    "    columns = cv_results.columns.tolist()\n",
    "    # rank_test_score first, mean_test_score second and std_test_score third\n",
    "    columns = columns[-1:] + columns[-3:-1] + columns[:-3]\n",
    "    cv_results = cv_results[columns]\n",
    "\n",
    "    if save:\n",
    "        cv_results.to_csv('--'.join(cv_results['params'][0].keys()) + '.csv', index=True, index_label='Id')\n",
    "\n",
    "    # Unless all_columns are True, drop not wanted columns: params, std_* split*\n",
    "    if not all_columns:\n",
    "        cv_results.drop('params', axis='columns', inplace=True)\n",
    "        cv_results.drop(list(cv_results.filter(regex='^std_.*')), axis='columns', inplace=True)\n",
    "        cv_results.drop(list(cv_results.filter(regex='^split.*')), axis='columns', inplace=True)\n",
    "\n",
    "    # Unless all_ranks are True, filter out those rows which have rank equal to one\n",
    "    if not all_ranks:\n",
    "        cv_results = cv_results[cv_results['rank_test_score'] == 1]\n",
    "        cv_results.drop('rank_test_score', axis = 'columns', inplace = True)        \n",
    "        cv_results = cv_results.style.hide_index()\n",
    "\n",
    "    display(cv_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_plots(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "\n",
    "scores(\"log_reg_opt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "y_test_pred = knn.predict(X_train)\n",
    "\n",
    "scores(\"KNN_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "hyperparameters = {'n_neighbors': np.arange(1, 21, 1),\n",
    "               'metric': [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "               'weights': ['uniform', 'distance']\n",
    "               }\n",
    "\n",
    "# Estimator for use in random search\n",
    "estimator = KNeighborsClassifier()\n",
    "\n",
    "# Create the random search model\n",
    "KNN = RandomizedSearchCV(estimator, hyperparameters,\n",
    "                         n_jobs=-1,\n",
    "                         scoring='recall',\n",
    "                         cv=5,\n",
    "                         n_iter=25,\n",
    "                         verbose=1)\n",
    "\n",
    "# Fit\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "print(KNN.best_params_)\n",
    "print(round(KNN.best_score_,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Best params on 07.October were: {'weights': 'distance', 'n_neighbors': 6, 'metric': 'euclidean'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_plots(KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores(\"KNN_opt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Baise GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gauss = GaussianNB()\n",
    "gauss.fit(X_train, y_train)\n",
    "y_pred = gauss.predict(X_test)\n",
    "y_test_pred = gauss.predict(X_train)\n",
    "\n",
    "scores(\"gaussian_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param grid\n",
    "hyperparameters = {\n",
    "    'var_smoothing': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]\n",
    "}\n",
    "\n",
    "# Estimator for use in random search\n",
    "estimator = GaussianNB()\n",
    "\n",
    "# Create the random search model\n",
    "gauss = RandomizedSearchCV(estimator, hyperparameters,\n",
    "                         n_jobs=-1,\n",
    "                         scoring='recall',\n",
    "                         cv=5,\n",
    "                         n_iter=30,\n",
    "                         verbose=1)\n",
    "\n",
    "# Fit\n",
    "gauss.fit(X_train, y_train)\n",
    "\n",
    "print(gauss.best_params_)\n",
    "print(round(gauss.best_score_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Best params on 07.October were: {'var_smoothing': 1e-15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_plots(gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores(\"gaussian_opt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "246.441px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
